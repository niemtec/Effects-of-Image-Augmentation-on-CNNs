{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Binary Classification using Convolutional Neural Networks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "An investigation into the effects that image augmentation has on the accuracy and loss of Convolutional Neural Networks. *This work was completed as part of dissertation project for Bachelor of Science (Honours) in Computer Science with specialism in Artificial Intelligence.*\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Libraries\n",
    "Import all the necessary libraries for the project to run. Primarily using TensorFlow base and Keras to construct the architecture.\n",
    "Also import supporting libraries like numpy for manipulating the arrays.\n",
    "\n",
    "The `classifier_helpers` library is a collection of helper functions extracted to make the code easier to read."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback, LearningRateScheduler\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import random\n",
    "import classifier_helpers as tools"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configuration Variables for Experiments\n",
    "Each of the variables controls a different area of the network structure. Collated here for easier control of changes between experiments.\n",
    "\n",
    "`results_file_name` defines the name of the output files for results etc.\n",
    "\n",
    "`dataset_path` points to the location of the dataset files (either images or arrays representing images).\n",
    "\n",
    "`rotation_range` the maximum rotational range of an image in either positive or negative direction (max 180Â°).\n",
    "\n",
    "`epochs` number of epochs to train the model for.\n",
    "\n",
    "`initial_learning_rate` the learning rate for the network to start off with, changing this can affect how quickly the model converges on the solution.\n",
    "\n",
    "`batch_size` number of samples to show the network before updating the network weights.\n",
    "\n",
    "`decay_rate` the rate at which the learning rate should decay over time\n",
    "\n",
    "`validation_dataset_size` percentage of the dataset to be used for testing, by default 75% goes to training and 25% goes to testing.\n",
    "\n",
    "`random_seed` random number to be used for seed - for repeatability of dataset shuffles.\n",
    "\n",
    "`image_depth` coloured images have three layers of depth.\n",
    "\n",
    "`results_path` path to the results directory\n",
    "\n",
    "`model_name` name of the model to be saved in the results file/model structure files\n",
    "\n",
    "`plot_name` titles/names for the result plots"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results_file_name = 'Batch-Size-2'\n",
    "dataset_path = '../dataset/'\n",
    "rotation_range = 0\n",
    "epochs = 100\n",
    "initial_learning_rate = 1e-5  # 1e-5\n",
    "batch_size = 2\n",
    "decay_rate = initial_learning_rate / epochs \n",
    "validation_dataset_size = 0.25\n",
    "random_seed = 42\n",
    "image_depth = 3\n",
    "\n",
    "results_path = 'results/'\n",
    "model_name = results_file_name + \"-\" + str(rotation_range)\n",
    "plot_name = model_name"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Define Helper Functions\n",
    "Additional functions used by the network, mainly controling decay rates for experiments with varying the decay of learning rates within the model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_lr_metric(optimizer):\n",
    "\tdef lr(y_true, y_pred):\n",
    "\t\treturn optimizer.lr\n",
    "\t\n",
    "\treturn lr\n",
    "\n",
    "def stepDecay(epoch):\n",
    "\tdropEvery = 10\n",
    "\tinitAlpha = 0.01\n",
    "\tfactor = 0.25\n",
    "\t# Compute learning rate for current epoch\n",
    "\texp = np.floor((1 + epoch) / dropEvery)\n",
    "\talpha = initAlpha * (factor ** exp)\n",
    "\t\n",
    "\treturn float(alpha)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Build the Network Architecture\n",
    "Compile the network architecture from individual layers. The function encapsulates the entire structure of the network which can be initialised as the model. It requires `width`, `height`, and `depth` values for the images it will be processing as well as the number of `classes` which it will be classifying. Binary classification requires that two classes are defined. (In this case benign and malignant samples)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def buildNetworkModel(width, height, depth, classes):\n",
    "\tmodel = Sequential()\n",
    "\tinput_shape = (height, width, depth)\n",
    "\t\n",
    "\t# If 'channel first' is being used, update the input shape\n",
    "\tif K.image_data_format() == 'channel_first':\n",
    "\t\tinput_shape = (depth, height, width)\n",
    "\t\n",
    "\t# First layer\n",
    "\tmodel.add(\n",
    "\t\tConv2D(20, (5, 5), padding = \"same\", input_shape = input_shape))  # Learning 20 (5 x 5) convolution filters\n",
    "\tmodel.add(Activation(\"relu\"))\n",
    "\tmodel.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2)))\n",
    "\t\n",
    "\t# Second layer\n",
    "\tmodel.add(Conv2D(50, (5, 5), padding = \"same\"))\n",
    "\tmodel.add(Activation(\"relu\"))\n",
    "\tmodel.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2)))\n",
    "\t\n",
    "\t# Third layer - fully-connected layers\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(50))  # 500 nodes\n",
    "\tmodel.add(Activation(\"relu\"))\n",
    "\t\n",
    "\t# Softmax classifier\n",
    "\tmodel.add(Dense(classes))  # number of nodes = number of classes\n",
    "\tmodel.add(Activation(\"softmax\"))  # yields probability for each class\n",
    "\t\n",
    "\t# Return the model\n",
    "\treturn model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load and Initialise the Dataset\n",
    "Load the dataset, normally the program can load images into its memory, however that is time consuming so instead the images have been loaded and exported as arrays for quicker load times.\n",
    "By using arrays, the dataset can be exported and loaded more efficiently within the Jupyter without the need to share the entire image library (~15GB).\n",
    "\n",
    "The array containing the images and their respective labels are loaded. They are then combined (so that the labels correspond to the image) and the shuffled whilst the label and image remain related.\n",
    "\n",
    "Randomised arrays are then split in accordance to the training-testing dataset split. By default it's set to 75% training and 25% testing. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sorted_data = np.load('sorted_data_array.npy')\n",
    "sorted_labels = np.load('sorted_labels_array.npy')\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "combined = list(zip(sorted_data, sorted_labels))\n",
    "random.shuffle(combined)\n",
    "data[:], labels[:] = zip(*combined)\n",
    "\n",
    "# Scale the raw pixel intensities to the range [0, 1]\n",
    "data = np.array(data, dtype = \"float\") / 255.0\n",
    "labels = np.array(labels)\n",
    "\n",
    "test_set = int(validation_dataset_size * len(labels))\n",
    "validation_dataset_labels = labels[-test_set:]\n",
    "\n",
    "# Partition the data into training and testing splits\n",
    "(train_x, test_x, train_y, test_y) = train_test_split(data, labels, test_size = test_set, random_state = random_seed)\n",
    "\n",
    "# Convert the labels from integers to vectors\n",
    "train_y = to_categorical(train_y, num_classes = 2)\n",
    "test_y = to_categorical(test_y, num_classes = 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Image Augmentation Generators\n",
    "Image augmentation generators are defined here, they take an input image and apply the predefined augmentation method, in this case `rotation_range` is applied to any image effectively rotating it to a random degree within that range."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_augmented_image_generator = ImageDataGenerator(rotation_range = rotation_range, fill_mode = \"nearest\")\n",
    "testing_augmented_image_generator = ImageDataGenerator(rotation_range = rotation_range, fill_mode = \"nearest\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compile the Network Model\n",
    "Compile the network model using the predefined structure from the `buildNetworkModel` and apply the optimiser and learning rate metrics.\n",
    "This is where we define the loss and accuracy metrics which are saved in the history dictionary."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(tools.stamp() + \"Compiling Network Model\")\n",
    "\n",
    "# Reducing the learning rate by half every 2 epochs\n",
    "learning_rate_schedule = [LearningRateScheduler(stepDecay)]\n",
    "\n",
    "# Build the model based on control variable parameters\n",
    "model = buildNetworkModel(width = 64, height = 64, depth = image_depth, classes = 2)\n",
    "\n",
    "# Set optimiser\n",
    "optimiser = Adam(lr = initial_learning_rate)\n",
    "lr_metric = get_lr_metric(optimiser)\n",
    "\n",
    "# Compile the model using binary crossentropy, preset optimiser and selected metrics\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer = optimiser, metrics = [\"accuracy\", \"mean_squared_error\", lr_metric])\n",
    "# Train the network\n",
    "print(tools.stamp() + \"Training Network Model\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save the Model\n",
    "Completed model can be saved to the disk along with all statistics and graphs. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save results of training in history dictionary for statistical analysis\n",
    "history = model.fit_generator(\n",
    "\ttraining_augmented_image_generator.flow(train_x, train_y, batch_size = batch_size),\n",
    "\tvalidation_data = (test_x, test_y),\n",
    "\tsteps_per_epoch = len(train_x) // batch_size,\n",
    "\tepochs = epochs,\n",
    "\tverbose = 1)\n",
    "\n",
    "# Save all runtime statistics and plot graphs\n",
    "tools.saveNetworkStats(history, epochs, initial_learning_rate, model_name, results_path)\n",
    "tools.saveAccuracyGraph(history, plot_name, results_path)\n",
    "tools.saveLossGraph(history, plot_name, results_path)\n",
    "tools.saveLearningRateGraph(history, plot_name, results_path)\n",
    "tools.saveModelToDisk(model, model_name, results_path)\n",
    "tools.saveWeightsToDisk(model, model_name, results_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}