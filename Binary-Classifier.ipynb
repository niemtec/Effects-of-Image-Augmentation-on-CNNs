{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Binary Classification using Convolutional Neural Networks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "An investigation into the effects that image augmentation has on the accuracy and loss of Convolutional Neural Networks. *This work was completed as part of dissertation project for Bachelor of Science (Honours) in Computer Science with specialism in Artificial Intelligence.*\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Packages\n",
    "Import all the necessary packages for the project to run."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras import backend as K\n",
    "import datetime\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Control Variables\n",
    "The below variables are used to control various elements of the network model. They have been concentrated in a single area to make experimental variation easier to achieve, they act as a \"control panel\" for the operation of the model.\n",
    "\n",
    "`datasetName` - name of the experiment performed (used for graph titles etc.)\n",
    "\n",
    "`resultsFileName` - prefix for all result filenames\n",
    "\n",
    "`rotationRange` - range of rotations for a given experiment (e.g. 0°, 45°, 90°, 135°, 180°)\n",
    "\n",
    "`categoryOne`, `categoryTwo` - name of first and second classification categories\n",
    "\n",
    "`modelName` - name of the model used for graphs and results (combination of the dataset name and current rotation range setting\n",
    "\n",
    "`datasetPath` - path to the image dataset master directory (containing sub-category directories)\n",
    "\n",
    "`resultsPath` - path to results directory\n",
    "\n",
    "`plotName` - title for all graph plots (by default using modelName)\n",
    "\n",
    "`graphSize` - dimensions of the graph plots\n",
    "\n",
    "`noEpochs` - number of epochs to run the model for\n",
    "\n",
    "`initialLearningRate` - the learning rate used for searching for problem space  optima\n",
    "\n",
    "`batchSize` - size of sample batches to be fed into the network model\n",
    "\n",
    "`decayRate` - the decay rate of the learning rate (by default dR = lR / E)\n",
    "\n",
    "`numberOfClases` - number of classification classes (binary classifier = 2)\n",
    "\n",
    "`validationDatasetSize` - size of the dataset to be used for validation/testing of the accuracy of the model (usually 25%)\n",
    "\n",
    "`randomSeed` - random number generation seed used for reproducibility\n",
    "\n",
    "`imageHeight`, `imageWidth` - dimensions of the input images\n",
    "\n",
    "`imageDepth` - Z dimension of the image (monochrome images = 1, colour = 3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "resultsFileName = 'Demo'\n",
    "datasetPath = 'Demo-dataset-rotation/'\n",
    "rotationRange = 135\n",
    "noEpochs = 100\n",
    "initialLearningRate = 1e-5\n",
    "batchSize = 32\n",
    "decayRate = initialLearningRate / noEpochs\n",
    "numberOfClasses = 2\n",
    "validationDatasetSize = 0.25\n",
    "randomSeed = 42\n",
    "imageHeight = 64\n",
    "imageWidth = 64\n",
    "imageDepth = 3\n",
    "\n",
    "resultsPath = 'Demo-results/'\n",
    "modelName = resultsFileName + \"-\" + str(rotationRange)\n",
    "plotName = modelName\n",
    "graphSize = (15, 10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def file_is_image(path_to_file):\n",
    "    filename, extension = os.path.splitext(path_to_file)\n",
    "    if extension != '.jpg':\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "# Prints current timestamp, to be used in print statements\n",
    "def stamp():\n",
    "    time = \"[\" + str(datetime.datetime.now().time()) + \"]   \"\n",
    "    return time\n",
    "\n",
    "\n",
    "# Save final model performance\n",
    "def save_network_stats(resultsPath, modelName, history, fileName, sensitivity, specificity, precision):\n",
    "    # Extract data from history dictionary\n",
    "    historyLoss = history.history['loss']\n",
    "    historyLoss = str(historyLoss[-1])  # Get last value from loss\n",
    "    historyAcc = history.history['acc']\n",
    "    historyAcc = str(historyAcc[-1])  # Get last value from accuracy\n",
    "    historyValLoss = history.history['val_loss']\n",
    "    # Get last value from validated loss\n",
    "    historyValLoss = str(historyValLoss[-1])\n",
    "    historyValAcc = history.history['val_acc']\n",
    "    # Get last value from validated accuracy\n",
    "    historyValAcc = str(historyValAcc[-1])\n",
    "    historyMSE = 0  # str(historyMSE[-1])\n",
    "    historyMAPE = 0  # history.history['mape']\n",
    "    historyMAPE = 0  # str(historyMAPE[-1])\n",
    "\n",
    "    with open(resultsPath + fileName + \".txt\", \"a\") as history_log:\n",
    "        history_log.write(\n",
    "            modelName + \",\" + historyLoss + \",\" + historyAcc + \",\" + historyValLoss + \",\" + historyValAcc + \",\" + str(\n",
    "                noEpochs) + \",\" + str(initialLearningRate) + \",\" + str(historyMSE) + \",\" + str(\n",
    "                historyMAPE) + \",\" + str(sensitivity) + \",\" + str(specificity) + \",\" + str(precision) + \"\\n\")\n",
    "    history_log.close()\n",
    "\n",
    "    print(stamp() + \"Keras Log Saved\")\n",
    "\n",
    "    print(history.history.keys())\n",
    "\n",
    "    print(stamp() + \"History File Saved\")\n",
    "\n",
    "\n",
    "# Build the network structure\n",
    "def build_network_model(width, height, depth, classes):\n",
    "    # Initialise the model\n",
    "    model = Sequential()\n",
    "    inputShape = (height, width, depth)\n",
    "\n",
    "    # If 'channel first' is being used, update the input shape\n",
    "    if K.image_data_format() == 'channel_first':\n",
    "        inputShape = (depth, height, width)\n",
    "\n",
    "    # First layer\n",
    "    model.add(\n",
    "        Conv2D(20, (5, 5), padding=\"same\", input_shape=inputShape))  # Learning 20 (5 x 5) convolution filters\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Second layer\n",
    "    model.add(Conv2D(50, (5, 5), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Third layer - fully-connected layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50))  # 500 nodes\n",
    "    model.add(Activation(\"relu\"))\n",
    "\n",
    "    # Softmax classifier\n",
    "    model.add(Dense(classes))  # number of nodes = number of classes\n",
    "    model.add(Activation(\"softmax\"))  # yields probability for each class\n",
    "\n",
    "    # Return the model\n",
    "    return model\n",
    "\n",
    "\n",
    "# Calculate confusion matrix statistics\n",
    "def calculate_statistics(tn, fp, fn, tp):\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (fp + tn)\n",
    "    precision = tp / (tp + fp)\n",
    "\n",
    "    return sensitivity, specificity, precision\n",
    "\n",
    "\n",
    "# Save the confusion matrix as a graphical figure\n",
    "def save_confusion_matrix(tp, tn, fp, fn):\n",
    "    import seaborn as sns\n",
    "    tp = int(tp)\n",
    "    tn = int(tn)\n",
    "    fp = int(fp)\n",
    "    fn = int(fn)\n",
    "\n",
    "    cm = [[tp, tn], [fp, fn]]\n",
    "    cm = np.array(cm)\n",
    "    heatmap = sns.heatmap(cm, annot=True, fmt='g', linewidths=0.2)\n",
    "    fig = heatmap.get_figure()\n",
    "    fig.savefig(resultsPath + '/' + modelName + '-confusion-matrix.png')\n",
    "\n",
    "\n",
    "# Summarize history for accuracy\n",
    "def save_accuracy_graph(history):\n",
    "    plt.figure(figsize=graphSize, dpi=75)\n",
    "    plt.grid(True, which='both')\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.suptitle(modelName)\n",
    "    plt.savefig(resultsPath + '/' + modelName + \"-accuracy.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Summarize history for loss\n",
    "def save_loss_graph(history):\n",
    "    plt.figure(figsize=graphSize, dpi=75)\n",
    "    plt.grid(True, which='both')\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.suptitle(modelName)\n",
    "    plt.savefig(resultsPath + '/' + modelName + \"-loss.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Initialize the data and labels arrays\n",
    "sortedData = []\n",
    "sortedLabels = []\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Go through dataset directory\n",
    "print(stamp() + \"Classifying the Dataset\")\n",
    "for datasetCategory in os.listdir(datasetPath):\n",
    "    datasetCategoryPath = datasetPath + \"/\" + datasetCategory\n",
    "\n",
    "    # Go through category 1 and then category 2 of the dataset\n",
    "    for sample in os.listdir(datasetCategoryPath):\n",
    "        # print(stamp() + sample)\n",
    "        if file_is_image(datasetCategoryPath + \"/\" + sample):\n",
    "            image = cv2.imread(datasetCategoryPath + \"/\" + sample)\n",
    "            image = cv2.resize(image, (\n",
    "                imageHeight, imageWidth))  # Network only accepts 28 x 28 so resize the image accordingly\n",
    "            image = img_to_array(image)\n",
    "            # Save image to the data list\n",
    "            sortedData.append(image)\n",
    "\n",
    "            # Decide on binary label\n",
    "            if datasetCategory == 'benign':\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 0\n",
    "            # Save label for the current image\n",
    "            sortedLabels.append(label)\n",
    "\n",
    "combined = list(zip(sortedData, sortedLabels))\n",
    "random.shuffle(combined)\n",
    "data[:], labels[:] = zip(*combined)\n",
    "\n",
    "# Scale the raw pixel intensities to the range [0, 1]\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "labels = np.array(labels)\n",
    "\n",
    "validationDatasetLabels = []\n",
    "# testSet = 0.25 * len(labels)\n",
    "validationDatasetLabels = labels[-7:]\n",
    "\n",
    "# Partition the data into training and testing splits\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=7,  # Set manually to 7 for Demo\n",
    "                                                  random_state=randomSeed)\n",
    "\n",
    "# Convert the labels from integers to vectors\n",
    "trainY = to_categorical(trainY, num_classes=numberOfClasses)\n",
    "testY = to_categorical(testY, num_classes=numberOfClasses)\n",
    "\n",
    "# Construct the image generator for data augmentation\n",
    "aug = ImageDataGenerator(\n",
    "    rotation_range=rotationRange,\n",
    "    fill_mode=\"nearest\"\n",
    ")\n",
    "\n",
    "augValidation = ImageDataGenerator(\n",
    "    rotation_range=rotationRange,\n",
    "    fill_mode=\"nearest\"\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "print(stamp() + \"Compiling Network Model\")\n",
    "\n",
    "# Build the model based on control variable parameters\n",
    "model = build_network_model(\n",
    "    width=imageWidth, height=imageHeight, depth=imageDepth, classes=numberOfClasses)\n",
    "\n",
    "# Set optimiser\n",
    "opt = Adam(lr=initialLearningRate, decay=decayRate)\n",
    "\n",
    "# Compile the model using binary crossentropy, preset optimiser and selected metrics\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=opt,\n",
    "              metrics=[\"accuracy\", \"mean_squared_error\", \"mean_absolute_error\"])\n",
    "# Train the network\n",
    "print(stamp() + \"Training Network Model\")\n",
    "\n",
    "# Save results of training in history dictionary for statistical analysis\n",
    "history = model.fit_generator(\n",
    "    aug.flow(trainX, trainY, batch_size=batchSize),\n",
    "    validation_data=(testX, testY),\n",
    "    steps_per_epoch=len(trainX) // batchSize,\n",
    "    epochs=noEpochs,\n",
    "    verbose=1)\n",
    "\n",
    "# The following can be used to produce confusion matrices if necessary\n",
    "# predictions = model.predict_classes(testX, batchSize, 0)\n",
    "# tn, fp, fn, tp = confusion_matrix(validationDatasetLabels, predictions).ravel()\n",
    "# print(tn, fp, fn, tp)\n",
    "sensitivity, specificity, precision = 0, 0, 0   # Set to 0 if not used\n",
    "# sensitivity, specificity, precision = calculate_statistics(tn, fp, fn, tp)\n",
    "\n",
    "# Save all runtime statistics and plot graphs\n",
    "save_network_stats(resultsPath, modelName, history,\n",
    "                   resultsFileName, sensitivity, specificity, precision)\n",
    "# save_confusion_matrix(tn, fp, fn, tp)\n",
    "save_accuracy_graph(history)\n",
    "save_loss_graph(history)\n",
    "\n",
    "# Save the model to disk\n",
    "print(stamp() + \"Saving Network Model\")\n",
    "model_json = model.to_json()\n",
    "with open(resultsPath + '/' + modelName + \".json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Save weights to disk\n",
    "print(stamp() + \"Saving Network Weights\")\n",
    "model.save_weights(resultsPath + '/' + modelName + \".h5\", \"w\")\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}